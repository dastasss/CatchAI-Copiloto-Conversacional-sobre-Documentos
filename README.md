# CatchAI ‚Äì Copiloto Conversacional sobre Documentos.

Este proyecto implementa un copiloto conversacional que permite a los usuarios subir hasta 5 archivos PDF y hacer preguntas en lenguaje natural sobre su contenido. La aplicaci√≥n utiliza t√©cnicas de Recuperaci√≥n Aumentada por Generaci√≥n (RAG) para proporcionar respuestas contextuales basadas en los documentos proporcionados.

## üöÄ C√≥mo Levantar el Entorno

### Prerrequisitos

Antes de continuar, aseg√∫rate de tener instalado lo siguiente en tu sistema:

1.  **[Docker Desktop](https://www.docker.com/products/docker-desktop/):** Necesario para construir y ejecutar la aplicaci√≥n contenedorizada.

2.  **[Ollama](https://ollama.com/):** La aplicaci√≥n depende de Ollama para ejecutar el modelo de lenguaje grande (LLM) de forma local. Sigue las instrucciones de instalaci√≥n para tu sistema operativo desde su sitio web.

    Una vez instalado, ejecuta el siguiente comando en tu terminal para descargar el modelo `llama3.2` que utiliza este proyecto:
    ```bash
    ollama pull llama3.2
    ```
    Aseg√∫rate de que la aplicaci√≥n de Ollama y Docker desktop se est√©n ejecutando en segundo plano antes de iniciar el contenedor de Docker.

### Pasos para la Ejecuci√≥n

1.  **Clonar el Repositorio:**
    ```bash
    git clone https://github.com/dastasss/CatchAI-Copiloto-Conversacional-sobre-Documentos..git
    
    ```
    

2.  **Construir y Ejecutar la Aplicaci√≥n con Docker Compose:**
    Desde la ra√≠z del proyecto (donde se encuentra `docker-compose.yml`), ejecuta el siguiente comando:
    ```bash
    docker-compose up --build
    ```
    *   La primera vez que ejecutes este comando, Docker descargar√° las im√°genes base, instalar√° las dependencias y construir√° la imagen de la aplicaci√≥n. Esto puede tardar unos minutos.
    *   Tambi√©n descargar√° los modelos de lenguaje de Hugging Face (`sentence-transformers/all-MiniLM-L6-v2` y `google/flan-t5-base`). Esto tambi√©n puede tomar tiempo dependiendo de tu conexi√≥n a internet.

3.  **Acceder a la Aplicaci√≥n:**
    Una vez que el contenedor est√© en funcionamiento, abre tu navegador web y ve a:
    ```
    http://localhost:8501
    ```

4.  **Detener la Aplicaci√≥n:**
    Para detener la aplicaci√≥n, presiona `Ctrl+C` en la terminal donde ejecutaste `docker-compose up`. Para eliminar los contenedores y la red, puedes usar:
    ```bash
    docker-compose down
    ```

## üèóÔ∏è Arquitectura del Sistema

La aplicaci√≥n sigue una arquitectura de **monolito contenedorizado** para simplificar el despliegue y la gesti√≥n, ideal para un proyecto de este tama√±o y alcance.

```
+---------------------+
|     Host Machine    |
|                     |
| +-----------------+ |
| | Docker Engine   | |
| |                 | |
| | +-------------+ | |
| | |  Container  | | |
| | |             | | |
| | |  Python App | | |
| | |  (Streamlit)| | |
| | |             | | |
| | | +---------+ | | |
| | | | LangChain | | |
| | | +---------+ | | |
| | | +---------+ | | |
| | | | HuggingF. | | |
| | | | Embeddings| | |
| | | +---------+ | | |
| | | +---------+ | | |
| | | | Ollama    | | |
| | | | (Host)    | | |
| | | +---------+ | | |
| | | +---------+ | | |
| | | | ChromaDB  | | |
| | | | (Persistent)| | |
| | | +---------+ | | |
| | +-------------+ | |
| +-----------------+ |
|                     |
| +-----------------+ |
| |  ./data/        | |  <-- Volumen compartido para PDFs
| |  (Host Dir)     | |      y posible persistencia de ChromaDB
+---------------------+
```

**Componentes Clave:**

*   **Streamlit:** Sirve como la interfaz de usuario (frontend) y el servidor de la aplicaci√≥n, manejando las interacciones del usuario y orquestando las llamadas a la l√≥gica de IA.
*   **LangChain:** Framework de orquestaci√≥n que facilita la construcci√≥n de la cadena RAG, integrando cargadores de documentos, divisores de texto, modelos de embeddings y LLMs.
*   **Hugging Face Embeddings:** Utiliza el modelo `sentence-transformers/all-MiniLM-L6-v2` para convertir los fragmentos de texto de los PDFs en representaciones num√©ricas (embeddings).
*   **Ollama:** Se utiliza para ejecutar modelos de lenguaje grandes (LLMs) como `llama3.2` localmente en la m√°quina host, proporcionando las capacidades de generaci√≥n de lenguaje natural.
*   **ChromaDB:** Base de datos vectorial ligera que almacena los embeddings de los documentos. Ahora persiste en el volumen `data/chroma_db` y se limpia autom√°ticamente al procesar nuevos documentos.
*   **Docker & Docker Compose:** Permiten empaquetar la aplicaci√≥n y todas sus dependencias en un contenedor aislado, asegurando que la aplicaci√≥n funcione de manera consistente en cualquier entorno.

## üí° Justificaci√≥n de Elecciones T√©cnicas

*   **Python:** Lenguaje est√°ndar en el ecosistema de IA, con una vasta colecci√≥n de librer√≠as y una comunidad activa.
*   **Streamlit:** Elegido por su rapidez para construir interfaces web interactivas con c√≥digo Python puro, ideal para prototipos y aplicaciones de demostraci√≥n. Simplifica la arquitectura al no requerir un backend y frontend separados.
*   **LangChain:** Proporciona abstracciones de alto nivel para construir flujos de trabajo de IA complejos como RAG, reduciendo la cantidad de c√≥digo boilerplate y facilitando la integraci√≥n de diferentes componentes.
*   **Ollama:** Elegido por su capacidad para ejecutar modelos de lenguaje grandes (LLMs) de forma local y gratuita, eliminando la dependencia de APIs de pago y proporcionando un entorno reproducible. Su facilidad de uso y gesti√≥n de modelos lo hacen ideal para este proyecto.
*   **ChromaDB:** Una base de datos vectorial sencilla y eficiente. Su integraci√≥n con LangChain es excelente, y ahora se gestiona para asegurar una limpieza autom√°tica al procesar nuevos documentos, garantizando que solo el contexto relevante est√© activo.
*   **Docker:** Garantiza la **reproducibilidad** del entorno. Cualquier persona con Docker puede levantar la aplicaci√≥n sin preocuparse por dependencias o configuraciones de sistema operativo.

## üí¨ Explicaci√≥n del Flujo Conversacional

1.  **Carga de Documentos:** El usuario sube archivos PDF a trav√©s de la interfaz de Streamlit.
2.  **Procesamiento:** Al hacer clic en "Procesar Documentos", la aplicaci√≥n:
    *   Guarda temporalmente los PDFs en el volumen `data/uploaded_pdfs`.
    *   Carga el contenido de los PDFs.
    *   Divide el texto en fragmentos (`chunks`) para un manejo eficiente.
    *   Genera embeddings (representaciones num√©ricas) para cada fragmento utilizando el modelo `all-MiniLM-L6-v2`.
    *   Almacena estos embeddings en una base de datos vectorial ChromaDB (en memoria).
3.  **Interacci√≥n Conversacional:** Una vez procesados, el usuario puede escribir preguntas en el chat.
4.  **Recuperaci√≥n (Retrieval):** Cuando el usuario hace una pregunta:
    *   La pregunta se convierte en un embedding.
    *   Se realiza una b√∫squeda en ChromaDB para encontrar los fragmentos de documentos m√°s relevantes a la pregunta.
5.  **Generaci√≥n (Generation):**
    *   La pregunta original del usuario y los fragmentos de texto recuperados se env√≠an al LLM (Ollama `llama3.2`) junto con un `prompt` espec√≠fico.
    *   El LLM utiliza este contexto para generar una respuesta coherente y basada en la informaci√≥n de los PDFs.
6.  **Visualizaci√≥n:** La respuesta del LLM se muestra en la interfaz de chat de Streamlit.

## ‚ú® Funcionalidad Opcional Implementada

Como parte de las mejoras, se ha a√±adido una funcionalidad opcional clave:

*   **Resumen de Documentos:** Se ha integrado un bot√≥n "Resumir Documentos" en la interfaz. Al hacer clic, la aplicaci√≥n genera un resumen conciso y autom√°tico para cada uno de los PDFs cargados, permitiendo al usuario obtener una visi√≥n general del contenido de forma r√°pida y eficiente.

## ‚ú® Screeshots de la app funcionando.

![Captura de pantalla de la interfaz](assets/images/1.png)

![Captura de pantalla de los resumenes](assets/images/2.png)

![Captura de pantalla de las preguntas](assets/images/3.png)



## üöß Limitaciones Actuales y Mejoras Futuras (Roadmap)

### Limitaciones Actuales:

*   **Dependencia de Ollama:** La aplicaci√≥n requiere que el servidor de Ollama est√© ejecut√°ndose en la m√°quina host y que el modelo `llama3.2` est√© descargado.
*   **Rendimiento del LLM:** La velocidad de respuesta del LLM (Ollama `llama3.2`) puede variar significativamente dependiendo de los recursos de hardware de la m√°quina host (RAM, CPU).
*   **Manejo de Errores:** La gesti√≥n de errores en la carga/procesamiento de PDFs es b√°sica.
*   **Interfaz de Usuario:** La interfaz es funcional pero simple, sin opciones avanzadas de personalizaci√≥n o gesti√≥n de documentos.
*   **Escalabilidad:** Dise√±ado para un √∫nico usuario y un volumen limitado de documentos.

### Mejoras Futuras (Roadmap):

1.  **Gesti√≥n de Persistencia de ChromaDB:** Aunque ChromaDB ya se limpia autom√°ticamente al procesar nuevos documentos, se podr√≠a a√±adir una opci√≥n en la interfaz de usuario para borrar la base de datos vectorial manualmente.
2.  **Selecci√≥n de Modelos:** Permitir al usuario elegir entre diferentes modelos de embeddings y LLMs (quiz√°s configurables por variables de entorno o una interfaz).
3.  **Funcionalidades Avanzadas:**
    
    *   **Comparaci√≥n de Documentos:** Implementar una funcionalidad para comparar autom√°ticamente informaci√≥n entre varios PDFs.
    *   **Clasificaci√≥n por Temas:** Desarrollar un m√≥dulo para clasificar los documentos o sus secciones por temas.
4.  **Manejo Avanzado de Errores:** Implementar un manejo de errores m√°s robusto y mensajes de usuario m√°s informativos.
5.  **Optimizaci√≥n de Rendimiento:** Explorar t√©cnicas para acelerar el procesamiento de documentos grandes o un mayor n√∫mero de PDFs.
6.  **Autenticaci√≥n/Multi-usuario:** Si el proyecto creciera, a√±adir un sistema de autenticaci√≥n para soportar m√∫ltiples usuarios.
7.  **Desacoplamiento de Backend:** Para una escalabilidad a gran escala, considerar separar la l√≥gica de RAG en un servicio de backend (FastAPI/Flask) y mantener Streamlit como solo el frontend.


## üèÅ Conclusi√≥n

Este proyecto demuestra con √©xito la viabilidad de un copiloto conversacional basado en RAG, empaquetado en una aplicaci√≥n interactiva y f√°cil de desplegar gracias a Streamlit y Docker. La arquitectura actual es ideal para la creaci√≥n r√°pida de prototipos, la validaci√≥n de conceptos y el uso a peque√±a escala.

Como siguiente paso hacia una aplicaci√≥n m√°s robusta, escalable y con una experiencia de usuario completamente personalizada, se recomienda una evoluci√≥n en su arquitectura. El camino ideal ser√≠a desacoplar el frontend del backend:

*   **Backend API:** Reconstruiria la l√≥gica de negocio como un servicio de API REST utilizando un framework s√≥lido como **Django, con Django REST Framework**. Esto permitir√≠a una gesti√≥n m√°s avanzada de la l√≥gica, los usuarios y la seguridad.
*   **Frontend Interactivo:** Desarrollaria una interfaz de usuario desde cero utilizando una librer√≠a moderna como **React**. Esto brindar√≠a control total sobre el dise√±o, la interactividad y la experiencia del usuario, permitiendo implementar una interfaz de chat pulida y a medida.

Esta arquitectura no solo alinear√≠a el proyecto con los est√°ndares de la industria para aplicaciones web modernas, sino que tambi√©n sentar√≠a las bases para futuras funcionalidades complejas y un mayor n√∫mero de usuarios.

---

```console
# Dev for:
# Daniel Mar√≠n Farias

<--Programador y Analista de Sistemas Jr.-->

# Contacto y Portafolio:
# Portfolio: https://porfolio-daniel.firebaseapp.com
# LinkedIn: https://www.linkedin.com/in/daniel-marin-farias
# GitHub: https://github.com/dastasss
```